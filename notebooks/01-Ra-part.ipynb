{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Israel Llorens <br/>\n",
    "> Lead Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 15:13:09 WARN Utils: Your hostname, Airon.local resolves to a loopback address: 127.0.0.1; using 192.168.100.57 instead (on interface en0)\n",
      "25/06/02 15:13:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/02 15:13:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\" # if you are running in macos Apple chip\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import openai\n",
    "import init\n",
    "\n",
    "from Framework.spark import spark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Suppress the specific FutureWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import hashlib\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag - Retrieval part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text and prepare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = spark.read.text('data/*.txt')\\\n",
    "    .withColumn(\"filename\", F.split(F.input_file_name(), os.sep).getItem( F.size(F.split(F.input_file_name(), os.sep))-1) )\\\n",
    "    .filter(F.col('value')!='')\n",
    "    \n",
    "# Trocear por frases\n",
    "sentence_df = text_df.withColumn(\n",
    "    \"sentence\", F.explode(F.split(F.col(\"value\"), r'(?<=[\\.\\!\\?])\\s+'))\n",
    ").drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, FloatType\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Define output schema\n",
    "schema = StructType([StructField(\"embedding\", ArrayType(FloatType()))])\n",
    "\n",
    "# Pandas UDF - load model inside to avoid broadcast\n",
    "@pandas_udf(schema)\n",
    "def embed_text(text_series: pd.Series) -> pd.DataFrame:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Load model inside UDF (once per executor)\n",
    "    if not hasattr(embed_text, 'local_model'):\n",
    "        embed_text.model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "        embed_text.tokenizer = AutoTokenizer.from_pretrained(embed_text.model_name)\n",
    "        embed_text.model = AutoModel.from_pretrained(embed_text.model_name)\n",
    "        embed_text.model.eval()\n",
    "    \n",
    "    local_tokenizer = embed_text.tokenizer\n",
    "    local_model = embed_text.model\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    local_model.to(device)\n",
    "    \n",
    "    try:\n",
    "        # Convert Series to list\n",
    "        texts = text_series.tolist()\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = local_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = local_model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            embeddings_np = embeddings.cpu().numpy().astype('float32')\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            \"embedding\": [vec.tolist() for vec in embeddings_np]\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in embed_text: {e}\")\n",
    "        return pd.DataFrame({\n",
    "            \"embedding\": [[0.0] * 1024] * len(text_series)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataframe\n",
    "sentence_df = sentence_df.withColumn('embedding', embed_text(\"sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|filename|            sentence|           embedding|\n",
      "+--------+--------------------+--------------------+\n",
      "| 163.txt|Further growth in...|{[0.026753383, 0....|\n",
      "| 163.txt|Germany: strong c...|{[-0.8266075, 0.4...|\n",
      "| 163.txt|T-Mobile US: syne...|{[0.23296036, 0.4...|\n",
      "| 163.txt|Europe: growth de...|{[-0.20402703, -0...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings to local FAISS vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = sentence_df.select(\"embedding\").rdd.flatMap(lambda x: x).collect()\n",
    "embeddings_np = np.array(embeddings_list, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([row.embedding for row in embeddings_list], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1024, Total vectors: 12355\n",
      "Adding vectors to index...\n",
      "Index total vectors: 12355\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Get dimension\n",
    "dimension = embeddings.shape[1]  # Should be 1024 for BGE-large\n",
    "print(f\"Embedding dimension: {dimension}, Total vectors: {embeddings.shape[0]}\")\n",
    "\n",
    "# Create HNSW index with L2 normalization for inner product\n",
    "M = 32  # Higher = more accurate but more memory\n",
    "faiss.normalize_L2(embeddings)  # Normalize for cosine similarity via inner product\n",
    "index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# Set efConstruction (controls build time/accuracy)\n",
    "index.hnsw.efConstruction = 40  # Typical: 40-100\n",
    "\n",
    "# Add vectors\n",
    "print(\"Adding vectors to index...\")\n",
    "index.add(embeddings)\n",
    "\n",
    "# Set efSearch at query time (higher = more accurate but slower)\n",
    "index.hnsw.efSearch = 64\n",
    "\n",
    "print(f\"Index total vectors: {index.ntotal}\")\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"embedding_vector_database.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12355"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n"
     ]
    }
   ],
   "source": [
    "collected_data = sentence_df.select(\"filename\", \"sentence\", \"embedding\").collect()\n",
    "\n",
    "# Extract embeddings and metadata\n",
    "filenames = [row.filename for row in collected_data]\n",
    "sentences = [row.sentence for row in collected_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "metadata = {\"filenames\": filenames, \"sentences\": sentences}\n",
    "with open(\"embedding_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the embedding from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model once globally\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_embedding(query):\n",
    "    \"\"\"Get embedding using pre-loaded model\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        query,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[0, 0, :]\n",
    "        embedding_np = embedding.cpu().numpy().astype('float32')\n",
    "    \n",
    "    return embedding_np.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Score: 0.6574\n",
      "   File: 165.txt\n",
      "   Text: Claudia Nemat, Member of the Board of Management for Technology, and Innovation will be reporting live from Barcelona on February 27 at 2:00 p.m.\n",
      "\n",
      "2. Score: 0.6352\n",
      "   File: 165.txt\n",
      "   Text: Schneier will discuss this in a talk with Telekom board member Claudia Nemat.\n",
      "\n",
      "3. Score: 0.6257\n",
      "   File: 74.txt\n",
      "   Text: Statements\n",
      "\n",
      "4. Score: 0.6233\n",
      "   File: 203.txt\n",
      "   Text: in their daily lives,\" explains Claudia Nemat, Board Member for Technology and Innovation at Telekom.\n",
      "\n",
      "5. Score: 0.6168\n",
      "   File: 158.txt\n",
      "   Text: Claudia Nemat will be reporting live from Barcelona with the \"Magenta Keynote\" on February 27, at 2:00 p.m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load index and metadata\n",
    "index = faiss.read_index(\"embedding_vector_database.faiss\")\n",
    "\n",
    "# Search example\n",
    "query = \"What does Claudia Nemat says\"\n",
    "query_embedding = get_embedding(query)  # Use same model\n",
    "faiss.normalize_L2(query_embedding)\n",
    "D, I = index.search(query_embedding, k=5)  # Top 5 results\n",
    "\n",
    "for i, (distance, idx) in enumerate(zip(D[0], I[0])):\n",
    "    print(f\"{i+1}. Score: {distance:.4f}\")\n",
    "    print(f\"   File: {filenames[idx]}\")\n",
    "    print(f\"   Text: {sentences[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use another technique, instead of FAISS ise Pinecone online service Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to insert and find your embeddings, we can use FAISS library, like we did before, or we can use the Free Tier from an online Vector Database named Pinecone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case scenario, I will be using OPENAI API KEY to generate the embeddings, and upload them into the Vector Database. All the needed coded is added to the Helper Framework inside the utils namespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone accessing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Framework.utils.pinecone_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    delete_all(index)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id, embedding, metadata = prepare_for_pinecone(['This is the first text uploaded to pinecone'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:   c07cc86f9c050186c54b14b81cdc6988 \n",
      "LEN:  1536 \n",
      "META: {'text': 'This is the first text uploaded to pinecone', 'date_uploaded': '2025-06-02 11:05:54.827826+00:00'}\n"
     ]
    }
   ],
   "source": [
    "print('ID:  ',_id, '\\nLEN: ', len(embedding), '\\nMETA:', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_texts_to_pinecone(['This is the first text uploaded to pinecone'], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'c07cc86f9c050186c54b14b81cdc6988',\n",
       "  'metadata': {'date_uploaded': '2025-06-02 11:08:25.570838+00:00',\n",
       "               'text': 'This is the first text uploaded to pinecone'},\n",
       "  'score': 0.419925719,\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_from_pinecone(\"This is a test\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "@F.udf(T.IntegerType())\n",
    "def text_to_pinecone(text, filename, api_key):\n",
    "    from Framework.utils.pinecone_helper import create_index, upload_texts_to_pinecone\n",
    "    \n",
    "    index = create_index(api_key)\n",
    "    upsert_count = upload_texts_to_pinecone([text], index, document=filename)\n",
    "    return upsert_count\n",
    "\n",
    "sentence_df = sentence_df.withColumn('n_uploaded', text_to_pinecone('sentence', 'filename', F.lit(os.environ.get('PINECONE_API_KEY'))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at this point we show the website pinecone.io, we will see each sentence being uploaded, with the document that it belongs to as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+\n",
      "|filename|            sentence|           embedding|n_uploaded|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "| 163.txt|Further growth in...|{[0.026753383, 0....|         1|\n",
      "| 163.txt|Germany: strong c...|{[-0.8266075, 0.4...|         1|\n",
      "| 163.txt|T-Mobile US: syne...|{[0.23296036, 0.4...|         1|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Framework.utils.pinecone_helper import create_index, delete_all\n",
    "try:\n",
    "    index = create_index()\n",
    "    delete_all(index)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.filter(F.length('sentence')>5).write.mode('overwrite').parquet('db_output');\n",
    "# sentence_df.filter(F.length('sentence')>5).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
